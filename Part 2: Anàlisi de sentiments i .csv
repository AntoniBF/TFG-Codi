
import os
from transformers import pipeline
from pathlib import Path
import re
import nltk
import csv

#Dues línies de codi per a importar els continguts de Google Drive
#from google.colab import drive
#drive.mount('/content/drive')

#Objecte path al drive. 
carpeta_articlesX = Path("/content/drive/MyDrive/")

#Path a modificar segons els textos que es vulguin analitzar
carpeta_a_analitzar = input("Nom de carpeta a analitzar:")
carpeta_anàlisi = carpeta_articlesX / f"{carpeta_a_analitzar}"

#Si les dades s'han extret amb el codi previ de web scraping, no cal modificar-ho.
FR = carpeta_anàlisi / "fr_paràgrafs"
DE = carpeta_anàlisi/ "de_paràgrafs"
EN = carpeta_anàlisi / "en_paràgrafs"
ES = carpeta_anàlisi/ "es_paràgrafs"

#Funció per a arrodonir resultats a la nsotra escala de treball en cas que s'hagin de calcular mitja d'alguns paràgrafs
def arrodoniment(x):
  #Valors de l'escala
  numeros_possibles = [0, 0.25, 0.5, 0.75]
  #Per exemple, abs()
  closest_value = min(numeros_possibles, key=lambda option: abs(option - x))

  return closest_value

#La funció analyze_documents_in_folder per a calcular els resultats amb el model d'anàlisi i guardar-los en una variable.
def analyze_documents_in_folder(folder_path):

  #Línia de codi de la pipeline del model d'anàlisi de sentiments
  sentiment_pipeline = pipeline(model="nlptown/bert-base-multilingual-uncased-sentiment")

  results = []
  for filename in os.listdir(folder_path):
    if filename.endswith(".txt"):  #Només llegim fitxers .txt.
      filepath = os.path.join(folder_path, filename)
      with open(filepath, "r") as f:
        text = f.read()

    #Contador de paraules
    tokenitzador=nltk.tokenize.RegexpTokenizer(r"[ldsmLDSM]\'|\w+|[^\w\s]+")
    tokens=tokenitzador.tokenize(text)

    #Separar el text si el contador de paraules és més gran que 350:
    if len(tokens) > 350:
                #Partim la quantitat de tokens en dues meitats:
                split_index = len(tokens) // 2
                #List splicing. list[start:end:step]
                text1 = " ".join(tokens[:split_index])  #Unim els tokens.
                text2 = " ".join(tokens[split_index:])

                #print(text1)
                result1 = sentiment_pipeline(text1)[0]["label"]

                result2 = sentiment_pipeline(text2)[0]["label"]

                #Llista en la qual posarem els dos resultats de cada meitat per a fer la mitja
                pre_Average = []


                etiqueta_a_numero = {"1 star": 0, "2 stars": 0.25, "3 stars": 0.5, "4 stars": 0.75, "5 stars": 1, "negative": 0, "neutral": 0.5, "positive": 1, "Very Negative": 0, "Negative": 0.25, "Neutral": 0.5, "Positive": 0.75, "Very Positive": 1, 0: 0, 0.25: 0.25, 0.5: 0.5, 1: 1}
                
                #Els numerical_score son les estrelles transformades en valors segons el diccionari.
                numerical_score = etiqueta_a_numero.get(result1)
                #Pre_Average és el valor de la primera meitat. 
                pre_Average.append(numerical_score)


                numerical_score2 = etiqueta_a_numero.get(result2)
                pre_Average.append(numerical_score2)
                #Mitjana entre els dos "score":
                average_sentiment = sum(pre_Average) / len(pre_Average)

                #Arrodoniment de la mitjana al valor més proper de la nostra escala (etiqueta_a_numero)
                final_sentiment = arrodoniment(average_sentiment)

                #Afegim el resultat a "results" amb la mateixa estructura de diccionari.                
                results.append({"filename": filename, "sentiment": [{"label": final_sentiment}]})

    #Si el text te menys de 350 tokens:
    else:

      result = sentiment_pipeline(text)
      results.append({"filename": filename, "sentiment": result})

  return results

#Funció per imprimir els resultats (hi ha molts items)
#i poder visualitzar els resultats durant l'execució. 
def printing(folder_path):
  etiqueta_a_numero = {"1 star": 0, "2 stars": 0.25, "3 stars": 0.5, "4 stars": 0.75, "5 stars": 1, "negative": 0, "neutral": 0.5, "positive": 1, "Very Negative": 0, "Negative": 0.25, "Neutral": 0.5, "Positive": 0.75, "Very Positive": 1, 0: 0, 0.25: 0.25, 0.5: 0.5, 1: 1}
  results = analyze_documents_in_folder(folder_path)
  results.sort(key=lambda item: int(re.search(r"\d", item['filename']).group()))

  #Extracció del nom de cada paràgraf i valor atorgat pel model
  for item in results:
    name = item["filename"]
    value = item["sentiment"][0]["label"]#["score"]
    numerical_score = etiqueta_a_numero.get(value)
    print(f"{name},{numerical_score}")

    #Per assegurar-se de l'estructura de l'item. Pot variar segons el model d'AS
    #print(item)
#Aquestes funcions tenen molt temps d'execució.
#printing(DE)
#printing(FR)
#printing(EN)
#printing(ES)


def resultats_a_csv(results, output_filename):
    etiqueta_a_numero = {"1 star": 0, "2 stars": 0.25, "3 stars": 0.5, "4 stars": 0.75, "5 stars": 1, "negative": 0, "neutral": 0.5, "positive": 1, "Very Negative": 0, "Negative": 0.25, "Neutral": 0.5, "Positive": 0.75, "Very Positive": 1, 0: 0, 0.25: 0.25, 0.5: 0.5, 1: 1}

    #Al diccionari {} podem guardar els resultats per llengua i número:
    data = {}
    max_paragraf_number = 0
    for item in results:
        language = item["filename"].split("_")[0] #Extreure la llengua del nom del document.
        paragraph_number = int(re.search(r"\d+", item["filename"]).group())  #Número paràgraf
        max_paragraf_number = max(max_paragraf_number, paragraph_number)

        if language not in data:
            data[language] = {}  #Fem un diccionari per llengua
        data[language][paragraph_number] = etiqueta_a_numero.get(item["sentiment"][0]["label"])  # Store the score

    
    #Escrivim les dades en un .csv per a processar les dades a posteriori.
    
    with open(output_filename, "w", newline="", encoding="utf-8") as csvfile:
        fieldnames = ["P"] + list(range(1, max_paragraf_number + 1))  #Columnes: Language + nº paràgrafs
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        for language, scores in data.items():
            row = {"P": language.upper()} #Canviem a majúscules per seguir el codi d'R
            row.update(scores)  #Afegir l'score de cada paràgraf
            writer.writerow(row)

#Definim una funció per a transformar les files en columnes.
#Serà més fàcil processar les dades en R.
def transposar (input_csv, transposat_csv):
    data = []
    
    with open(input_csv, 'r', newline='', encoding='utf-8') as input:
        reader = csv.reader(input)
        for row in reader:
            data.append(row)
    #Juntem els elements d'una mateixa columna de totes les files. 
    transposed_data = list(zip(*data))

    with open(transposat_csv, 'w', newline='', encoding='utf-8') as outfile:
        writer = csv.writer(outfile)
        writer.writerows(transposed_data)

#Definim una funció per analitzar tots els documents i guardar-los en .csv alhora
def anàlisi_simultani(folder_paths, output_filename):
    all_results = []
    #folder_paths és una llista de tots els fitxers que volem analitzar; loop
    for folder_path in folder_paths:
        results = analyze_documents_in_folder(folder_path)
        all_results.extend(results)

    resultats_a_csv(all_results, output_filename)
    transposar(output_filename, output_filename.replace(".csv", "_dades.csv"))

#Crida de la funció:

#Primer hem de fer una llista de paths els quals corresponguin als texts d'anàlisi. És a dir, les subcarpetes de la part 1.
folder_paths = [DE, FR, EN, ES]

#Posar el nom desitjat del fitxer output.
#Obtindrem 2 .csv a sample_data: un en horitzontal i un altre en vertical. 
anàlisi_simultani(folder_paths, f"{carpeta_a_analitzar}.csv")
