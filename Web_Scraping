{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras sentiment analysis with Elmo Embeddings\n",
    "\n",
    "One of the recent trends in Natural Language Processing is transfer learning. Transfer learning allows NLP models to learn more from fewer examples. In this notebook, we experiment with so-called [ELMo Embeddings](https://allennlp.org/elmo), a new approach to word embeddings that relies on a large unlabelled text corpus to understand word meaning in context. ELMo Embeddings are available from [Tensorflow Hub](https://alpha.tfhub.dev/google/elmo/2)."
   ]
  },


import requests
from bs4 import BeautifulSoup
import pathlib
from pathlib import Path
import re

#Introduim els URLs dels contes en cada llengua
urls = [
    "https://www.grimmstories.com/en/grimm_fairy-tales/aschenputtel", 
    "https://www.grimmstories.com/es/grimm_cuentos/la_cenicienta", 
    "https://www.grimmstories.com/fr/grimm_contes/cendrillon",
    "https://www.grimmstories.com/de/grimm_maerchen/aschenputtel"
]

#Input per a crear el nom de la carpeta d'emmagatzematgesegons el nom del conte
input_nom = input("Introdueix el nom del conte:")
carpeta_articles = Path(f"/Users/tonibf/Desktop/patata/{input_nom}")
carpeta_articles.mkdir(parents=True, exist_ok=True)


def contes(contes, carpeta_articles):

    for url in urls:
        #Extracció de les llengües a partir dels links. /en/, /fr/...
        llengua = re.findall(r"/\w\w/", str(url))
        #Variable per a guardar la llengua del conte i utilitzar-la en la f-string
        llengua2 = "".join(re.findall(r"\w\w", str(llengua)))
        
        #Creació de la carpeta que conté els paràgrafs
        parapgraph_folder = carpeta_articles / f"{str(llengua2)}_paràgrafs"
        parapgraph_folder.mkdir(parents=True, exist_ok=True)

        #Perquè la web no ens consideri bots (o disminuir la probabilitat que ho faci):
        headers = {
    "User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/117.0"
}
        response = requests.get(url, headers=headers)
        response.raise_for_status() 
        
        #Creació d'un document amb el conte sencer
        file_path = carpeta_articles / f"{str(llengua2)}_Conte_Sencer.txt"
        with file_path.open ("w", encoding="utf-8") as x:

                
                soup = BeautifulSoup(response.content, 'html.parser')

                
                text = soup.find_all('div', {"class": "s"})

                for p in text:
                    x.write(p.get_text() + '\n\n')

                
        for i, div in enumerate(text, start=1):
            output_file = parapgraph_folder / f"{llengua2}_Prgrf_{i}.txt"
            with output_file.open("w", encoding="utf-8") as f:
                f.write(div.get_text(strip=True)) 
contes(urls, carpeta_articles)
