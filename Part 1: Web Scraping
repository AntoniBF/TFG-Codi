#Moduls necessaris per a crear la funció de web scraping.
import requests
from bs4 import BeautifulSoup
import pathlib
from pathlib import Path
import re

#Llista amb els URLs dels contes que volem analitzar(Per exemple, La Ventafocs).
urls = [
    "https://www.grimmstories.com/en/grimm_fairy-tales/aschenputtel", 
    "https://www.grimmstories.com/es/grimm_cuentos/la_cenicienta", 
    "https://www.grimmstories.com/fr/grimm_contes/cendrillon",
    "https://www.grimmstories.com/de/grimm_maerchen/aschenputtel"
]

#Input per a crear el nom de la carpeta d'emmagatzematge segons el nom del conte
input_nom = input("Introdueix el nom del conte:")

#Carpeta on es guardaran els contes i paràgrafs.  A modificar segons els directoris de treball de l'usuari.
carpeta_articles = Path(f"/Users/tonibf/Desktop/patata/{input_nom}")
carpeta_articles.mkdir(parents=True, exist_ok=True)

#Comencem a definir la funció
def contes(urls, carpeta_articles):

    for url in urls:
        #Extracció de les llengües a partir del format dels links: /en/, /fr/...
        llengua = re.findall(r"/\w\w/", str(url))
        #Variable per a guardar la llengua del conte i utilitzar-la en la f-string
        llengua2 = "".join(re.findall(r"\w\w", str(llengua)))
        
        #Creació de la carpeta que conté els paràgrafs
        carpeta_paragrafs = carpeta_articles / f"{str(llengua2)}_paràgrafs"
        carpeta_paragrafs.mkdir(parents=True, exist_ok=True)

        #Perquè la web no ens consideri bots (o disminuir la probabilitat que ho faci):
        headers = {
    "User-Agent": "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:109.0) Gecko/20100101 Firefox/117.0"
}
        response = requests.get(url, headers=headers)
        response.raise_for_status() 
        
        #Creació d'un document amb el conte sencer
        file_path = carpeta_articles / f"{str(llengua2)}_Conte_Sencer.txt"
        with file_path.open ("w", encoding="utf-8") as conte:

                
                soup = BeautifulSoup(response.content, 'html.parser')

                #<div = class "s"> és el header que marca el contingut i paràgrafs dels contes al codi HTML de grimmmstories.com.
                #Per tant, text és tot el contingut dins aquest tipus de header. 
                text = soup.find_all('div', {"class": "s"})
                
                #Per cada header, escrivim el contingut al document del conte sencer i introduim dos salts de línia per separar els paràgrafs.
                for p in text:
                    #Amb .get_text ignorem totes les etiquetes HTML.
                    conte.write(p.get_text() + '\n\n')

        #Enumerate() itera "text": [contingut, índex]
        #l'index, i, l'utilitzem per enumerar els paràgrafs al nom del document
        #contingut és el text, el qual l'escrivim al document corresponent.
        for i, contingut in enumerate(text, start=1):
            #i dins l'f-string permet enumrar els documents.
            output_file = carpeta_paragrafs / f"{llengua2}_Prgrf_{i}.txt"
            with output_file.open("w", encoding="utf-8") as paragraf:
                
                paragraf.write(contingut.get_text(strip=True)) 
#Cridem la funció. La primera varaible una llista d'URLS i la segona el directori on es guardaran els texts.
contes(urls, carpeta_articles)
